{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaSingh1907/Credit-Card-Default-Prediction/blob/main/Credit_Card_Default_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name** - **Credit Card Default Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -** Aditya Singh\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "The project focuses on predicting credit card payment defaults among customers in Taiwan. Rather than a simple binary classification, the emphasis is on estimating the probability of default. This approach provides a more nuanced understanding of credit risk, enhancing risk management practices.\n",
        "\n",
        "The dataset underwent rigorous preprocessing, including handling missing values, addressing outliers, categorical encoding, and feature engineering. Exploratory data analysis (EDA) provided crucial insights, revealing correlations between payment history, bill amounts, and default status. Visualizations, such as heatmaps and histograms, were used to identify trends and patterns.\n",
        "\n",
        "Multiple machine learning models were trained and evaluated, including Logistic Regression, Random Forest, and XG Boosting. Hyperparameter tuning and cross-validation were performed to optimize model performance. The final selected model, XG Boosting, demonstrated the highest test accuracy of 85% and an AUC of 0.853.\n",
        "\n",
        "The K-S chart was employed to evaluate the estimated probability of default. This tool proved invaluable in identifying customers at higher risk of defaulting on their credit card payments, providing a more accurate risk assessment.\n",
        "\n",
        "\n",
        "\n",
        "**Technical Documentation:**\n",
        "\n",
        "\n",
        "\n",
        "**1. Introduction:**\n",
        "\n",
        "Background and Problem Statement: The project aims to predict credit card payment defaults in Taiwan, a critical task for effective risk management.\n",
        "\n",
        "Objectives and Goals: The primary objective is to estimate the probability of default rather than relying on a binary classification, allowing for a more nuanced assessment of credit risk.\n",
        "\n",
        "Scope and Deliverables: The project focuses on data analysis, preprocessing, modeling, and model evaluation.\n",
        "\n",
        "**2. Data Description:**\n",
        "\n",
        "Data Source: The dataset comprises credit card transaction information from Taiwan. It includes features related to payment history, bill amounts, and demographic information.\n",
        "Data Preprocessing: This stage involved addressing missing values, treating outliers, encoding categorical variables, and performing feature engineering. It ensured the dataset was ready for modeling.\n",
        "\n",
        "**3. Exploratory Data Analysis (EDA):**\n",
        "\n",
        "Summary Statistics: Descriptive statistics provided an overview of the dataset, highlighting key measures like means, medians, and standard deviations.\n",
        "Data Visualizations: Various visualizations, including heatmaps and histograms, revealed important insights about feature distributions and relationships.\n",
        "\n",
        "**4. Modeling:**\n",
        "\n",
        "Model Selection: Logistic Regression, Random Forest, and XG Boosting were chosen for their suitability in addressing credit risk assessment. Each model underwent training and evaluation.\n",
        "Hyperparameter Tuning: Through cross-validation, hyperparameters were optimized to enhance model performance.\n",
        "\n",
        "**5. Results and Conclusion:**\n",
        "\n",
        "Model Performance: The final selected model, XG Boosting, demonstrated an 85% test accuracy and an AUC of 0.853. It outperformed other models in accurately predicting credit card defaults.\n",
        "Insights: The K-S chart proved instrumental in identifying high-risk customers, contributing to a more refined credit risk assessment.\n",
        "\n",
        "**6. Future Work:**\n",
        "\n",
        "Areas for Improvement: Further feature engineering and exploring additional modeling techniques could potentially enhance model performance.\n",
        "Potential Enhancements: The project could benefit from incorporating external economic indicators or demographic data to refine predictions."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This project is aimed at predicting the case of customers' default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the K-S chart to evaluate which customers will default on their credit card payments.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O, data manipulation\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "from pandas import set_option\n",
        "plt.style.use('ggplot') # nice plots\n",
        "\n",
        "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
        "from sklearn.linear_model import LogisticRegression # to apply the Logistic regression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import KFold # for cross validation\n",
        "from sklearn.model_selection import GridSearchCV # for tuning parameter\n",
        "from sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.\n",
        "from sklearn.preprocessing import StandardScaler # for normalization\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn import metrics # for the check the error and accuracy of the model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import os\n",
        "#print(os.listdir(\"../input\"))\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e_udhONzet0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path ='/content/drive/MyDrive/default of credit card clients.xls - Data.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qR6u47SIdqIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(),cbar=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abuot My dataset ,it consists of 30001 rows and 25 columns without any duplicate or missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   ID: Unique identifier for each customer.\n",
        "2.   LIMIT_BAL: Credit limit for the customer.\n",
        "3.   SEX: Gender of the customer (1 = Male, 2 = Female).\n",
        "4.   EDUCATION: Education level of the customer (1 = Graduate School, 2 = University, 3 = High School, 4 = Others).\n",
        "5.  MARRIAGE: Marital status of the customer (1 = Married, 2 = Single, 3 = Others).\n",
        "6.   AGE: Age of the customer.\n",
        "7.   PAY_X: Payment status for the month X, where X ranges from 0 to 6 (e.g., PAY_0 for the most recent month).\n",
        "8.   BILL_AMT_X: Bill amount for the month X, where X ranges from 1 to 6.\n",
        "9.   PAY_AMT_X: Payment amount for the month X, where X ranges from 1 to 6.\n",
        "10.  default payment next month: Binary indicator of whether the customer defaulted on payment in the next month (1 = Default, 0 = No Default).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create a copy of the current dataset and assigning to df\n",
        "df=df.copy()\n",
        "\n",
        "#remove the id column\n",
        "df.drop('ID', axis = 1, inplace =True) # drop column \"ID\""
      ],
      "metadata": {
        "id": "U5gsG7tqa1Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create a copy of the current dataset and assigning to df\n",
        "df=df.copy()\n",
        "#renaming of columns\n",
        "df.rename(columns={'default payment next month' : 'Defaulter'}, inplace=True)\n",
        "df.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "df.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "df.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for columns name\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IZQW3rqvv1Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing values with there labels\n",
        "df.replace({'SEX': {1 : 'Male', 2 : 'Female'}}, inplace=True)\n",
        "df.replace({'EDUCATION' : {1 : 'Graduate School', 2 : 'University', 3 : 'High School', 4 : 'Others'}}, inplace=True)\n",
        "df.replace({'MARRIAGE' : {1 : 'Married', 2 : 'Single', 3 : 'Others'}}, inplace = True)\n",
        "df.replace({'Defaulter': {1 : 'Yes', 0: 'No'}},inplace = True)"
      ],
      "metadata": {
        "id": "MyXDcbrowCiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for replaced labels\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5cWyMHIDwu8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#category wise values\n",
        "df['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "zeCWVKjWAE8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   In education column, values such as 5,6 and 0 are unknown. Lets combine those values as others."
      ],
      "metadata": {
        "id": "VC7H0WryBsZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replcae values with 5, 6 and 0 to Others\n",
        "df.EDUCATION = df.EDUCATION.replace({5: \"Others\", 6: \"Others\",0: \"Others\"})"
      ],
      "metadata": {
        "id": "gop2y88UA0xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#category wise values\n",
        "df['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "IetgeEKHBYRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  In marriage column, 0 values are unknown. Combine those values in others category.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ21gwquB7Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replace 0 with Others\n",
        "df.MARRIAGE = df.MARRIAGE.replace({0: \"Others\"})"
      ],
      "metadata": {
        "id": "ri9xJYh7BfFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I've performed several data manipulations on our dataset, including renaming columns and replacing values with their corresponding labels. These are common steps in data wrangling to make the data more understandable and suitable for analysis and renaming columns related to payment history, bill amounts, and payment amounts, as well as converting numeric values to descriptive labels for categorical variables.\n",
        "\n",
        " As for insights, I've performed, but renaming columns and converting values to labels usually help make the data more interpretable. This can enhance your understanding of the dataset and potentially make it easier to communicate findings to others."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Mapping the target: categorizing"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "## The frequency of defaults\n",
        "yes = (df['Defaulter'] == 'Yes').sum()\n",
        "no = (df['Defaulter'] == 'No').sum()\n",
        "\n",
        "# Percentage\n",
        "yes_perc = round(yes / len(df) * 100, 1)\n",
        "no_perc = round(no / len(df) * 100, 1)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.set_context('notebook', font_scale=1.2)\n",
        "\n",
        "# Define the order of categories explicitly\n",
        "order = ['No', 'Yes']\n",
        "\n",
        "# Use 'order' parameter to specify the order of categories\n",
        "sns.countplot(x='Defaulter', data=df, palette=\"Blues\", order=order)\n",
        "\n",
        "# Annotate with counts and percentages\n",
        "plt.annotate('Non-defaulter: {}'.format(no), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\n",
        "plt.annotate('Defaulter: {}'.format(yes), xy=(0.7, 15000), xytext=(0.7, 3000), size=12)\n",
        "plt.annotate(str(no_perc)+\" %\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\n",
        "plt.annotate(str(yes_perc)+\" %\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\n",
        "plt.title('COUNT OF CREDIT CARDS', size=14)\n",
        "# Removing the frame\n",
        "plt.box(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ctKEpEc7top"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the choice of this specific chart (count plot) was driven by its ability to clearly represent the distribution of categorical data in our dataset, aligning well with my intention to visualize the frequency of defaulters."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this sample of 30,000 credit card holders, there are 6,636 default credit cards; that is, the proportion of default in the data is 22.1%, and number of non-defaulter credit cards is 23364 that's ,the proportion of non-defaulter in the data is 77.9%."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gained Insights into credit card defaults can improve risk management, customize offerings, and enhance fraud detection. therefor, we can say that these insights help us to creating a positive business impact."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Distribution of all Numerical Variables (Univariant Analysis)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Defining the numeric features\n",
        "numeric_features = ['LIMIT_BAL','AGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR','PAY_AMT_SEPT','PAY_AMT_AUG','PAY_AMT_JUL','PAY_AMT_JUN','PAY_AMT_MAY','PAY_AMT_APR','Defaulter']\n",
        "\n",
        "for col in numeric_features[:-1]:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = df[col]\n",
        "    feature.hist(bins=50, ax=ax)\n",
        "    ax.axvline(feature.mean(), color='red', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.set_title(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As histogram is a very popular tool so the chart will show the overview of each and every variables informayion and gives a clear idea about the data set. it also sumarizes the measured data."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that the data is positively skewed, indicating an elongated right tail in the distribution."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, understanding that the data is positively skewed is important for making informed business decisions. It suggests that there may be a concentration of data on the lower end with some high-value outliers. This insight can influence strategies related to risk assessment, resource allocation, and potentially lead to more targeted marketing efforts or tailored product offerings for specific customer segments.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 -Frequency of explanatory variables by defaulted and non-defaulted cards"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Creating a new dataframe with categorical variables\n",
        "subset = df[['SEX', 'EDUCATION', 'MARRIAGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR','Defaulter']]\n",
        "\n",
        "f, axes = plt.subplots(3, 3, figsize=(20, 15), facecolor='white')\n",
        "f.suptitle('FREQUENCY OF CATEGORICAL VARIABLES (BY TARGET)')\n",
        "ax1 = sns.countplot(x=\"SEX\", hue=\"Defaulter\", data=subset, palette=\"Blues\", ax=axes[0,0])\n",
        "ax2 = sns.countplot(x=\"EDUCATION\", hue=\"Defaulter\", data=subset, palette=\"Blues\",ax=axes[0,1])\n",
        "ax3 = sns.countplot(x=\"MARRIAGE\", hue=\"Defaulter\", data=subset, palette=\"Blues\",ax=axes[0,2])\n",
        "ax4 = sns.countplot(x='PAY_SEPT', hue=\"Defaulter\", data=subset, palette=\"Blues\", ax=axes[1,0])\n",
        "ax5 = sns.countplot(x=\"PAY_AUG\", hue=\"Defaulter\", data=subset, palette=\"Blues\", ax=axes[1,1])\n",
        "ax6 = sns.countplot(x=\"PAY_JUL\", hue=\"Defaulter\", data=subset, palette=\"Blues\", ax=axes[1,2])\n",
        "ax7 = sns.countplot(x=\"PAY_JUN\", hue=\"Defaulter\", data=subset, palette=\"Blues\", ax=axes[2,0])\n",
        "ax8 = sns.countplot(x=\"PAY_MAY\", hue=\"Defaulter\", data=subset, palette=\"Blues\", ax=axes[2,1])\n",
        "ax9 = sns.countplot(x=\"PAY_APR\", hue=\"Defaulter\", data=subset, palette=\"Blues\", ax=axes[2,2]);"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart is a grid of count plots, and it is chosen for visualizing the frequency distribution of categorical variables with respect to the \"Defaulter\" target variable. This choice of visualization is appropriate because count plots are effective in comparing the distribution of categorical variables across different categories."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The categorical variables being analyzed are \"SEX,\" \"EDUCATION,\" \"MARRIAGE,\" and the payment status variables (\"PAY_SEPT\" to \"PAY_APR\") and the frequency of non-defaulter higher then the frequncy of defaulters in each section and when talk about all sections indivisually like, sex(male, female)comapair male,the frequency of non-defaulter in female is higher then male,education,married or pay section"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights from the categorical variables can potentially help in creating a positive business impact. These insights provide\n",
        "a deeper understanding of the distribution and characteristics of key variables related to credit card usage and payment behavior.\n",
        "\n",
        "For instance, understanding the average credit limit, age distribution, payment status trends, bill amounts, and payment amounts can assist in tailoring credit card offerings, setting credit limits, and designing targeted marketing strategies. This information can also contribute to the development of more accurate risk assessment models, helping the business make informed decisions regarding credit approvals and managing default risks.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 -Distribution of Credit Limits by Default Status"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Histogram plot using Seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=df, x='LIMIT_BAL', hue='Defaulter', multiple='stack', bins=30)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel(\"Credit Limit\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Credit Limits by Default Status\")\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(title=\"Default\", labels=['Non-Defaulter', 'Defaulter'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms are used to display the distribution of a single variable. They help understand the frequency and spread of data within specific ranges or bins. In our case, using a histogram to show the distribution of credit limits for different default statuses provides insights into how credit limits are distributed among defaulters and non-defaulters."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining the histogram, we can observe the distribution of credit limits for different default status.\n",
        "Defaulters tend to have a more varied distribution across credit limit ranges, while non-defaulters show a more concentrated distribution.\n",
        "This indicates that credit limits might play a role in predicting default behavior, but other factors could also be influential.\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "these insights enable businesses to manage risks more effectively, offer tailored services, and make strategic decisions that positively impact their bottom line and customer satisfaction."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 -Distribution of Bill amounts with Payment Amounts and along with dependent variable"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Distribution of bill amount by previous payment amounts and 'defaulter' status\n",
        "plt.figure(figsize=(30, 10))\n",
        "months = ['SEPT', 'AUG', 'JUL', 'JUN', 'MAY', 'APR']\n",
        "titles = ['September', 'August', 'July', 'June', 'May', 'April']\n",
        "\n",
        "for i, month in enumerate(months):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    sns.scatterplot(data=df, x='PAY_AMT_' + month, y='BILL_AMT_' + month, hue='Defaulter', palette=\"Set1\")\n",
        "    plt.xlabel(\"Payment Amount\")\n",
        "    plt.ylabel(\"Bill Amount\")\n",
        "    plt.title(\"Bill Amount vs Payment Amount - \" + titles[i])\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plots comparing bill amounts to payment amounts for each specific month were chosen because they effectively show the relationship between these two variables. This visualization allows us to understand how customers' payment behavior corresponds to their bill amounts. The separation of plots for defaulters and non-defaulters provides a clear comparison and highlights any potential patterns or trends"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot find any particular pattern when we compare bill amount and pay amount of each month, except for may be, that the number of defaulters decreses among customers with higher pay amount, as we move from April to September."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 -Payment History Trends"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Selecting payment delay columns for visualization\n",
        "payment_columns = ['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Define colors for defaulters and non-defaulters\n",
        "colors = {'Yes': 'red', 'No': 'blue'}\n",
        "\n",
        "# Plotting the trend of payment delays for defaulters and non-defaulters\n",
        "for status in ['Yes', 'No']:\n",
        "    for column in payment_columns:\n",
        "        subset = df[df['Defaulter'] == status]\n",
        "        plt.plot(subset[column].value_counts().sort_index(), label=status + \" - \" + column, color=colors[status])\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel(\"Payment Delay\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Payment History Trends\")\n",
        "plt.xticks(range(-2, 9))  # Adjust the range based on your data\n",
        "plt.legend(title=\"Defaulter\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the specific chart of a line chart showing the trend of payment delays over the months for defaulters and non-defaulters because it helps in understanding how payment behavior changes over time. This visualization allows us to observe any patterns or trends in payment delays, helping us identify potential risk factors or predictive indicators related to credit card defaults."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the \"Payment History Trends\" line chart, we can observe the following insights:\n",
        "\n",
        "Payment Behavior: Both defaulters and non-defaulters tend to have payment delays in the first few months, particularly in the first two to three months. This suggests that customers, regardless of their default status, might face difficulties in making payments on time during the initial period.\n",
        "\n",
        "Improvement Over Time: As the months progress, there is a general trend of improving payment behavior. The frequency of payment delays decreases over time, indicating that customers become more regular in making payments.\n",
        "\n",
        "Differences Between Defaulters and Non-Defaulters: Defaulters consistently show higher frequencies of payment delays across all months compared to non-defaulters. This highlights that payment behavior, especially delayed payments, is a distinguishing factor between the two groups."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from visualizations can positively impact business by improving risk management, enabling personalized offerings, enhancing fraud detection, and aiding in strategic decision-making."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the Defaulter value by 0 and 1.\n",
        "df.replace({'Defaulter': {'Yes' : 1, 'No': 0}},inplace = True)"
      ],
      "metadata": {
        "id": "csHzt9nPLird"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "corr = df.corr()\n",
        "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
        "\n",
        "def magnify():\n",
        "    return [dict(selector=\"th\",\n",
        "                 props=[(\"font-size\", \"7pt\")]),\n",
        "            dict(selector=\"td\",\n",
        "                 props=[('padding', \"0em 0em\")]),\n",
        "            dict(selector=\"th:hover\",\n",
        "                 props=[(\"font-size\", \"12pt\")]),\n",
        "            dict(selector=\"tr:hover td:hover\",\n",
        "                 props=[('max-width', '200px'),\n",
        "                        ('font-size', '12pt')])\n",
        "]\n",
        "\n",
        "corr.style.background_gradient(cmap, axis=1)\\\n",
        "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
        "    .set_caption(\"Hover to magify\")\\\n",
        "    .set_precision(2)\\\n",
        "    .set_table_styles(magnify())"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap is a suitable choice when we want to understand the relationships between different numeric variables in a dataset. By visualizing the correlations using a heatmap, we can quickly identify patterns of positive or negative relationships between variables. This helps in revealing potential multicollinearity or dependencies among features, which is valuable for tasks such as feature selection, identifying redundant variables, or understanding potential influences on target variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmat shows that features are correlated with each other (collinearity), such as like PAY_SEPT,AUG,JUL,JUN,MAY,APR and BILL_AMT_SEPT,AUG,JUL,JUN,MAY,APR. In those cases, the correlation is positive."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Selecting a subset of columns for pair plot\n",
        "pair_columns = ['LIMIT_BAL', 'AGE', 'BILL_AMT_SEPT', 'PAY_AMT_SEPT', 'Defaulter']\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(data=df[pair_columns], hue='Defaulter', diag_kind='kde', palette='Set1')\n",
        "\n",
        "# Adding a legend\n",
        "plt.legend(title=\"Defaulter\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because it's a useful way to visualize relationships between multiple variables in our dataset. Pair plots display scatter plots for numerical variables and histograms or kernel density estimates for single variables along the diagonal. This helps identify potential correlations, patterns, and distributions among the variables, providing a comprehensive overview of the data's relationships."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I hvae used defaulter in the hue variable so the above plot will show the distribution of defaulter and non-defaulters with different type of columns."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for further feature engineering\n",
        "df=df.copy()"
      ],
      "metadata": {
        "id": "iTc4VvZgYQ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the outliers\n",
        "\n",
        "# Defining the numeric features\n",
        "numeric_features = ['LIMIT_BAL','AGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR','PAY_AMT_SEPT','PAY_AMT_AUG','PAY_AMT_JUL','PAY_AMT_JUN','PAY_AMT_MAY','PAY_AMT_APR','Defaulter']\n",
        "\n",
        "# Create a figure and adjust the size\n",
        "fig = plt.figure(figsize=(16, 32))\n",
        "\n",
        "# Initialize the counter 'c'\n",
        "c = 1\n",
        "\n",
        "# Loop through numeric features\n",
        "for i in numeric_features:\n",
        "    plt.subplot(7, 3, c)\n",
        "    plt.xlabel('Distribution of {}'.format(i))\n",
        "    sns.boxplot(x=i, data=df, color=\"purple\")\n",
        "    c += 1\n",
        "\n",
        "# Adjust the layout of subplots\n",
        "plt.tight_layout(pad=8.4, w_pad=9.5, h_pad=5.0)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Selecting only numerical columns\n",
        "numerical_columns = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculating Z-scores\n",
        "z_scores = zscore(numerical_columns)\n",
        "\n",
        "# Creating a DataFrame to store Z-scores\n",
        "z_scores_df = pd.DataFrame(z_scores, columns=numerical_columns.columns)\n",
        "\n",
        "# Identifying potential outliers using the specified threshold\n",
        "outliers = z_scores_df.abs() > 3\n",
        "\n",
        "# Printing the count of potential outliers for each column\n",
        "print(outliers.sum())"
      ],
      "metadata": {
        "id": "RChk9rYgp0kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the numeric features\n",
        "numeric_features_log = ['LIMIT_BAL','AGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR','PAY_AMT_SEPT','PAY_AMT_AUG','PAY_AMT_JUL','PAY_AMT_JUN','PAY_AMT_MAY','PAY_AMT_APR','Defaulter']\n",
        "\n",
        "# Applying Log transformation\n",
        "log_transformed_data = np.log1p(df[numeric_features_log])"
      ],
      "metadata": {
        "id": "X7B4v5Tk65Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only numerical columns\n",
        "numerical_columns_log = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculating Z-scores\n",
        "z_scores = zscore(numerical_columns_log)\n",
        "\n",
        "# Creating a DataFrame to store Z-scores\n",
        "z_scores_df = pd.DataFrame(z_scores, columns=numerical_columns_log.columns)\n",
        "\n",
        "# Identifying potential outliers using the specified threshold\n",
        "outliers = z_scores_df.abs() > 3\n",
        "\n",
        "# Printing the count of potential outliers for each column\n",
        "print(outliers.sum())"
      ],
      "metadata": {
        "id": "Wwo3t06R9hdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I have calculated Z-scores for our numerical columns in a DataFrame and identified potential outliers using a threshold of 3 for the absolute Z-score.\n",
        "\n",
        "then I've applied a log transformation for Handling Outliers & Outlier treatments. This transformation is used to reduce the impact of extreme values and make the distribution of the data more symmetrical. It's particularly useful when dealing with positively skewed data, as taking the logarithm tends to \"compress\" the higher values.\n",
        "\n",
        "This technique helps in making the data more suitable for models that assume a more normal distribution."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Converting the value to 0 or 1\n",
        "encoders_nums = {\"SEX\":{\"Female\":0,\"Male\":1}, \"Defaulter\":{\"Yes\":1,\"No\":0}}\n",
        "df = df.replace(encoders_nums)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "tHvPJLER8NY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply dummification (one-hot encoding)\n",
        "df = pd.get_dummies(df,columns = [\"EDUCATION\",\"MARRIAGE\"])\n",
        "\n",
        "# Display the DataFrame with dummy columns\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "KFaz22wa6JIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "S33K-CdBUtEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['EDUCATION_Others', 'MARRIAGE_Others'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "61Sd8kNLUyIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dummy variables by droping firs variable\n",
        "#df = pd.get_dummies(df, columns = ['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR'], drop_first = True )"
      ],
      "metadata": {
        "id": "oKSOPDK7awUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "mMbPj7Pga26t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for all the created variables\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CiH7_-UWa7ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used dummification (one-hot encoding) technique .One-hot encoding is chosen because it helps prevent the algorithm from assuming a natural order among the categories and provides a straightforward way to represent categorical information.I used this technique to convert categorical columns into a format suitable for machine learning algorithms that require numerical input."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I already have deleted the 'ID' column from the dataset as that information is not important for the analysis."
      ],
      "metadata": {
        "id": "nnTV39UPbtxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "corr= df.corr()\n",
        "plt.figure(figsize=(25,10))\n",
        "sns.heatmap(corr,annot=True, cmap=plt.cm.Accent_r)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmat shows that features are correlated with each other (collinearity), such us like PAY_SEPT,AUG,JUL,JUN,MAY,APR and BILL_AMT_SEPT,AUG,JUL,JUN,MAY,APR. In those cases, the correlation is positive."
      ],
      "metadata": {
        "id": "4EyHUeOuiQEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "7t7iPDyiaOPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries for data transformation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#seperating dependant and independant variabales\n",
        "X = df.drop(labels='Defaulter', axis=1)\n",
        "y = df['Defaulter']\n"
      ],
      "metadata": {
        "id": "zS2ddwcx6Jyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the shape of X and Y\n",
        "print(f\"The Number Rows and Columns in X is {X.shape} Respectively.\")\n",
        "print(f\"The Number Rows and Columns in Y is {y.shape} Respectively.\")"
      ],
      "metadata": {
        "id": "FWeLRMUz7rvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "BasnwK3bYdUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the numerical features\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled"
      ],
      "metadata": {
        "id": "bZFap4mwyG4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used StandardScaler method to scale our data .StandardScaler is a good choice for scaling your data when you want to ensure that your features have similar scales and are centered around zero. This can help improve the performance of our machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SkNdoGnnyXjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "qvq4wLHG1DhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "aUm8eu_bFujV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Defaulter'].value_counts()"
      ],
      "metadata": {
        "id": "E0D3J2flJdGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "sns.countplot(x = 'Defaulter', data = df)"
      ],
      "metadata": {
        "id": "uATMW0dWVUW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handaling imbalance dataset using SMOTE (if needed)\n",
        "#importing SMOTE to handle class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sm = SMOTE()\n",
        "\n",
        "#fit predictor and target variable\n",
        "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "print('Original unbalanced dataset shape', len(df))\n",
        "print('Resampled balanced dataset shape', len(y_train))"
      ],
      "metadata": {
        "id": "4U-eGgyrEELo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new dataframe from balanced dataset after SMOTE\n",
        "balanced_df = pd.DataFrame(X_train, columns=list(i for i in list(df.describe(include='all').columns) if i != 'Defaulter'))"
      ],
      "metadata": {
        "id": "rW1OE7dxQXEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding target variable to new created dataframe\n",
        "balanced_df['Defaulter'] = y_train"
      ],
      "metadata": {
        "id": "wkMJuXiPXlSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of balanced dataframe\n",
        "balanced_df.shape"
      ],
      "metadata": {
        "id": "doemVwMGYqoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To display upto 200 columns and rows at once\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 200)"
      ],
      "metadata": {
        "id": "3b7xjGZMYza5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation among all the features\n",
        "balanced_df.corr()"
      ],
      "metadata": {
        "id": "buvWtm9mY64V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seperating dependant and independant variabales\n",
        "X = balanced_df[(list(i for i in list(balanced_df.describe(include='all').columns) if i != 'Defaulter'))]\n",
        "y = balanced_df['Defaulter']"
      ],
      "metadata": {
        "id": "QI-eYqTocPTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries for data transformation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Eq-wMpeMKD9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries for splitting data into training and testing dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "q-YoLE4TKKn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why?"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Synthetic Minority Over-sampling Technique (SMOTE) to handle the class imbalance in the dataset. SMOTE generates synthetic samples for the minority class by interpolating between existing instances. This technique helps to balance the class distribution and prevent the model from being biased towards the majority class.\n",
        "\n",
        "In my case SMOTE is used to oversample the minority class, creating a balanced dataset. By doing this, I ensure that the model can learn from both classes equally, leading to more accurate predictions and better generalization. The countplot at the end of the code confirms that the dataset is now balanced, which is essential for training a fair and unbiased model."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 -Logistic Regression Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# ML Model - 1 Implementation\n",
        "logi = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "\n",
        "# Fit the Algorithm\n",
        "logi.fit(X_train,y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients\n",
        "logi.coef_"
      ],
      "metadata": {
        "id": "7_1edHWBNVK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept value\n",
        "logi.intercept_"
      ],
      "metadata": {
        "id": "2RlqNZbXNcJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class prediction of y\n",
        "y_pred_logi = logi.predict(X_test)\n",
        "y_train_pred_logi=logi.predict(X_train)"
      ],
      "metadata": {
        "id": "soRxvB6Rc0QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting all scores for Logistic Regression\n",
        "train_accuracy_logi = round(accuracy_score(y_train_pred_logi,y_train), 3)\n",
        "accuracy_logi = round(accuracy_score(y_pred_logi,y_test), 3)\n",
        "precision_score_logi = round(precision_score(y_pred_logi,y_test), 3)\n",
        "recall_score_logi = round(recall_score(y_pred_logi,y_test), 3)\n",
        "f1_score_logi = round(f1_score(y_pred_logi,y_test), 3)\n",
        "roc_score_logi = round(roc_auc_score(y_pred_logi,y_test), 3)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_logi)\n",
        "print(\"The accuracy on test data is \", accuracy_logi)\n",
        "print(\"The precision on test data is \", precision_score_logi)\n",
        "print(\"The recall on test data is \", recall_score_logi)\n",
        "print(\"The f1 on test data is \", f1_score_logi)\n",
        "print(\"The roc_score on test data is \", roc_score_logi)\n"
      ],
      "metadata": {
        "id": "Kv6QZ6yS3yWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "all_lassifiers = ['Logistic Regression']\n",
        "train_accuracy = [train_accuracy_logi]\n",
        "test_accuracy = [accuracy_logi]\n",
        "precision_score = [precision_score_logi]\n",
        "recall_score = [recall_score_logi]\n",
        "f1_score = [f1_score_logi]\n",
        "auc_score = [roc_score_logi]\n",
        "\n",
        "model_report = pd.DataFrame(data={'model':all_lassifiers, 'Train Accuracy': train_accuracy, 'Test Accuracy': test_accuracy, 'Precision': precision_score, 'Recall': recall_score, 'F1 Score':f1_score , 'AUC': auc_score})\n",
        "\n",
        "model_report"
      ],
      "metadata": {
        "id": "28fOvRcJNyPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "# Get the confusion matrix\n",
        "cm_logi = confusion_matrix(y_test, y_pred_logi )\n",
        "print(cm_logi)\n",
        "\n",
        "#plot confusion matrix\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm_logi, annot=True, ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Logistic Regression')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learning model used in our analysis is logistic regression. Logistic regression is a binary classification algorithm that predicts the probability of a binary outcome (in our case, whether a credit card will default or not).\n",
        "\n",
        "\n",
        "*    The accuracy on the train data is 0.815, which indicates that the model's predictions match the actual labels to a good extent on the training data.\n",
        "\n",
        "*  The accuracy on the test data is 0.81, suggesting that the model performs well on unseen data and generalizes reasonably.\n",
        "\n",
        "* The precision on the test data is 0.71, indicating that among the instances the model predicted as positive, around 71.7% are truly positive.\n",
        "\n",
        "* The recall on the test data is 0.88, suggesting that the model effectively captures around 88.1% of the actual positive instances.\n",
        "*   The F1-score on the test data is 0.79, which is the harmonic mean of precision and recall. It gives a balanced measure of the model's accuracy on positive predictions.\n",
        "*  The ROC AUC score on the test data is 0.82, showing the model's ability to distinguish between positive and negative cases.\n",
        "\n",
        "\n",
        "Overall, the model seems to be performing well, with a relatively high recall indicating that it's capturing a significant portion of actual positive instances. However, the precision could be improved to reduce false positives.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EURLPujjCsfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# penalty in Logistic Regression Classifier\n",
        "penalties = ['l2', 'none']\n",
        "\n",
        "# hyperparameter C\n",
        "C= [0.0001, 0.001, 0.1, 0.5, 0.75, 1, 1.25, 1.5, 5, 10]\n",
        "\n",
        "# Hyperparameter Grid\n",
        "param_dict = {'penalty': penalties,\n",
        "              'max_iter': [100, 1000, 2500, 5000],\n",
        "              'C': C}"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Logistic Regression\n",
        "logi = LogisticRegression()\n",
        "\n",
        "# Grid search\n",
        "logi_grid = GridSearchCV(estimator=logi,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=3, n_jobs = -1, scoring='roc_auc')\n",
        "# fitting model\n",
        "logi_grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "HjOjAEP9aDbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logi_grid.best_estimator_"
      ],
      "metadata": {
        "id": "C1yimfBnzyt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logi_optimal_model =logi_grid.best_estimator_"
      ],
      "metadata": {
        "id": "0lTjCO1jTddv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_test_pred_logi_grid = logi_optimal_model.predict(X_test)\n",
        "y_train_pred_logi_grid = logi_optimal_model.predict(X_train)\n"
      ],
      "metadata": {
        "id": "MjsQ7J2fUEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_train_pred_logi_grid, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, y_train_pred_logi_grid))"
      ],
      "metadata": {
        "id": "JPOlecVGp7P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_test_pred_logi_grid , y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, y_test_pred_logi_grid))"
      ],
      "metadata": {
        "id": "QGyIMQmeUdc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrices for train and test\n",
        "train_cm_logi_grid = confusion_matrix(y_train,y_train_pred_logi_grid)\n",
        "test_cm_logi_grid = confusion_matrix(y_test,y_test_pred_logi_grid )"
      ],
      "metadata": {
        "id": "FNfzzug-HOgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cm_logi_grid"
      ],
      "metadata": {
        "id": "OB2H_T2SHXdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cm_logi_grid"
      ],
      "metadata": {
        "id": "5ZIn6bibHbT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "Grid Search is a popular choice for hyperparameter optimization due to its simplicity, comprehensiveness, and ability to integrate with cross-validation. It allows you to systematically explore different hyperparameter values and select the combination that yields the best model performance."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 -Random Forest Classification"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting data into Random Forest Classifier\n",
        "rfc=RandomForestClassifier(n_estimators=50)\n",
        "# Fit the Algorithm\n",
        "rfc.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3EtHH0WMg9Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class prediction of y\n",
        "y_pred_rfc=rfc.predict(X_test)\n",
        "y_train_pred_rfc=rfc.predict(X_train)"
      ],
      "metadata": {
        "id": "vC_4tds71LOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting all scores for Random Forest Classifier\n",
        "# Calculating accuracy on train and test\n",
        "train_accuracy = accuracy_score(y_train,y_train_pred_rfc)\n",
        "test_accuracy = accuracy_score(y_test,y_pred_rfc)\n",
        "\n",
        "print(\"The accuracy on train dataset is\", train_accuracy)\n",
        "print(\"The accuracy on test dataset is\", test_accuracy)"
      ],
      "metadata": {
        "id": "OPSDTB0z2BEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Get the confusion matrix for Random Forest Classifier\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm_rfc = confusion_matrix(y_test, y_pred_rfc )\n",
        "print(cm_rfc)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm_rfc, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "uq9aAHi12L3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm_rfc = confusion_matrix(y_train, y_train_pred_rfc )\n",
        "print(cm_rfc)\n",
        "\n",
        "#plot confusion matrix\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm_rfc, annot=True, ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix - Random Forest Classifier')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "-uhdPuRc2NrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_train_pred_rfc , y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, y_train_pred_rfc ))"
      ],
      "metadata": {
        "id": "rQFmUFC4xnOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_pred_rfc, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, y_pred_rfc))"
      ],
      "metadata": {
        "id": "lLg8RiOhxyrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(i for i in list(balanced_df.describe(include='all').columns) if i != 'Defaulter')\n",
        "feature_importances_rfc = rfc.feature_importances_\n",
        "feature_importances_rfc_df = pd.Series(feature_importances_rfc, index=features)\n",
        "feature_importances_rfc_df.sort_values(ascending=False)[0:15]"
      ],
      "metadata": {
        "id": "FAsam18oMkXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model that I've used is a powerful ensemble learning algorithm based on decision trees. It builds multiple decision trees during training and combines their predictions to provide more accurate and robust results. Here's the model's performance of evaluation metric score charts:\n",
        "\n",
        "Training Set Performance:\n",
        "\n",
        "Precision and Recall: The classification report for the training set indicates that the model has achieved a perfect precision and recall score of 1.00 for both classes (0 and 1). This suggests that the model has accurately classified all instances in the training set. However, achieving such perfect scores might indicate potential overfitting.\n",
        "\n",
        "F1-Score: The F1-score is also 1.00 for both classes, further indicating that the model performs exceptionally well on the training data. However, again, this could be a sign of overfitting.\n",
        "\n",
        "ROC AUC Score: The ROC AUC score of 0.9996 suggests that the model's ability to discriminate between the positive and negative classes is extremely high on the training data.\n",
        "\n",
        "Test Set Performance:\n",
        "\n",
        "Precision and Recall: The classification report for the test set shows that the model's precision and recall are slightly lower than perfect but still quite high. The precision is around 0.89 for class 0 and 0.83 for class 1, indicating that when the model predicts a positive class, it's correct approximately 89% and 83% of the time, respectively. The recall is around 0.84 for class 0 and 0.89 for class 1, indicating that the model correctly identifies approximately 84% and 89% of the actual positive instances.\n",
        "\n",
        "F1-Score: The F1-scores for both classes are around 0.87 and 0.86, respectively. These scores balance precision and recall, giving a measure of the overall performance on the test data.\n",
        "\n",
        "ROC AUC Score: The ROC AUC score of 0.864 suggests that the model's ability to discriminate between positive and negative classes is strong, although not as high as in the training set."
      ],
      "metadata": {
        "id": "zULNV4rn86GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Importance**"
      ],
      "metadata": {
        "id": "Dd5s2VaKv59C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "feature_importances_rfc_df.nlargest(15).plot(kind='bar')\n",
        "plt.xlabel(\"Features\", fontsize=12)\n",
        "plt.ylabel(\"Coefficient\", fontsize=12)\n",
        "plt.title('Feature Importance', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aoYISJQozIeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "rfc = RandomForestClassifier()\n",
        "# Number of trees\n",
        "n_estimators = [100,150,200]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [10,20,30]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# Hyperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "# Fit the Algorithm\n",
        "# Grid search\n",
        "rfc_grid = GridSearchCV(estimator=rfc,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='roc_auc')\n",
        "# fitting model\n",
        "rfc_grid.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_grid.best_estimator_"
      ],
      "metadata": {
        "id": "3A6T2UJQJOmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_grid.best_params_"
      ],
      "metadata": {
        "id": "k42PXvfFJSje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_optimal_model = rfc_grid.best_estimator_"
      ],
      "metadata": {
        "id": "l6PGu4MwJV8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_rfc_grid=rfc_optimal_model.predict(X_test)\n",
        "y_train_pred_rfc_grid=rfc_optimal_model.predict(X_train)\n"
      ],
      "metadata": {
        "id": "I7Ff7nIBI3E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_pred_rfc_grid , y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, y_pred_rfc_grid))"
      ],
      "metadata": {
        "id": "0j9BmvaWJLiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_train_pred_logi_grid, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, y_train_pred_logi_grid))"
      ],
      "metadata": {
        "id": "OddUFY341xtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrices for train and test\n",
        "train_cm_rfc_grid = confusion_matrix(y_train,y_train_pred_rfc_grid)\n",
        "test_cm_rfc_grid = confusion_matrix(y_test,y_pred_rfc_grid )"
      ],
      "metadata": {
        "id": "LKn06DolJily"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cm_rfc_grid"
      ],
      "metadata": {
        "id": "iL14i_KrJmGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cm_rfc_grid"
      ],
      "metadata": {
        "id": "q2ZRE19KJwkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "Grid Search is a popular choice for hyperparameter optimization due to its simplicity, comprehensiveness, and ability to integrate with cross-validation. It allows you to systematically explore different hyperparameter values and select the combination that yields the best model performance."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 -Implementing XgBoost Classifier"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "#fitting data into XG Boosting Classifier\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgb.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_xgb=xgb.predict(X_test)\n",
        "y_train_pred_xgb=xgb.predict(X_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm_xgb= confusion_matrix(y_test, y_pred_xgb)\n",
        "print(cm_xgb)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm_xgb, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_train, y_train_pred_xgb)\n",
        "print(cm_xgb)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "poBmbBMs3Jzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_train_pred_xgb, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, y_train_pred_xgb))"
      ],
      "metadata": {
        "id": "a-v9xoUo3efE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_pred_xgb, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "id": "RlYwBTgQ3rgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I used XGBoost algorithm to create the model. As I got there good result.\n",
        "\n",
        "For training dataset, i found precision of 89% and recall of 82% and f1-score of 85% and for Non-de BUt, I am also interested to see the result for Churning cutomer result as I got precision of 46% and recall of 95% and f1-score of 62%. Accuracy is 92% and average percision, recall & f1_score are 73%, 93% and 79% respectively with a roc auc score of 72%.\n",
        "\n",
        "For testing dataset, i found precision of 99% and recall of 90% and f1-score of 94% for False Churn customer data. BUt, I am also interested to see the result for Churning cutomer result as I got precision of 35% and recall of 80% and f1-score of 48%. Accuracy is 90% and average percision, recall & f1_score are 67%, 85% and 71% respectively with a roc auc score of 66%.\n",
        "\n",
        "Next tryting to improving the score by using hyperparameter tuning technique."
      ],
      "metadata": {
        "id": "1y0zMZVD4Gi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Importance**"
      ],
      "metadata": {
        "id": "IxHFl-yQwIX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(i for i in list(balanced_df.describe(include='all').columns) if i != 'Defaulter')\n",
        "feature_importances_xgb = xgb.feature_importances_\n",
        "feature_importances_xgb_df = pd.Series(feature_importances_rfc, index=features)\n"
      ],
      "metadata": {
        "id": "beES-47y4INi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances_xgb_df.sort_values(ascending=False)[0:15]"
      ],
      "metadata": {
        "id": "a6bWcunN4S-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,7))\n",
        "feature_importances_xgb_df.nlargest(15).plot(kind='bar')\n",
        "plt.xlabel(\"Features\", fontsize=12)\n",
        "plt.ylabel(\"Coefficient\", fontsize=12)\n",
        "plt.title('Feature Importance', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wC_Ghv6z4Xlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "xgb= XGBClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Grid search\n",
        "xgb_grid = GridSearchCV(estimator=xgb,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2, scoring='roc_auc')\n",
        "\n",
        "xgb_grid.fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "\n",
        "#class prediction of y on train and test\n",
        "y_pred_xgb_grid=xgb_grid.predict(X_test)\n",
        "y_train_pred_xgb_grid=xgb_grid.predict(X_train)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best: %f using %s\" % (xgb_grid.best_score_, xgb_grid.best_params_))"
      ],
      "metadata": {
        "id": "f4jBEDaF7unR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_train, y_train_pred_xgb_grid)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "AQUeC4SZ79Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(y_train_pred_xgb, y_train))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_train, y_train_pred_xgb))\n",
        "\n",
        "\n",
        "print(metrics.classification_report(y_pred_xgb_grid, y_test))\n",
        "print(\" \")\n",
        "\n",
        "print(\"roc_auc_score\")\n",
        "print(metrics.roc_auc_score(y_test, y_pred_xgb_grid))"
      ],
      "metadata": {
        "id": "PxveOlXu9EB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "Grid Search is a popular choice for hyperparameter optimization due to its simplicity, comprehensiveness, and ability to integrate with cross-validation. It allows you to systematically explore different hyperparameter values and select the combination that yields the best model performance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I would like to go with both Recall and Precision and which describes both is F1 Score.\n",
        "It balances both false positives and false negatives and can be a good overall indicator when the class distribution is imbalanced and I want a metric that reflects the model's overall effectiveness in capturing true positives while minimizing false positives and false negatives."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Model Comparision\n",
        "models = [\n",
        "    'Logistic Regression',\n",
        "'Optimal Logistic Regression',\n",
        "    'XG Boosting',\n",
        "    'Optimal XG Boosting',\n",
        "    'Random Forest',\n",
        "    'Optimal Random Forest'\n",
        "]\n",
        "\n",
        "train_accuracy = [0.81,0.82, 0.92, 0.92, 1.00, 0.83]\n",
        "test_accuracy = [0.80,0.81, 0.85, 0.85, 0.86, 0.83]\n",
        "precision_score = [0.71,0.72, 0.81, 0.88, 1.00, 0.79]\n",
        "recall_score = [0.87,0.89, 0.88, 0.95, 1.00, 0.86]\n",
        "f1_score = [0.79,0.80, 0.84, 0.92, 1.00, 0.82]\n",
        "auc_score = [0.819,0.815, 0.846, 0.853, 0.9996, 0.831]\n",
        "\n",
        "# Create a DataFrame to store the model evaluation metrics\n",
        "model_report = pd.DataFrame(data={\n",
        "    'Model': models,\n",
        "    'Train Accuracy': train_accuracy,\n",
        "    'Test Accuracy': test_accuracy,\n",
        "    'Precision': precision_score,\n",
        "    'Recall': recall_score,\n",
        "    'F1 Score': f1_score,\n",
        "    'AUC': auc_score\n",
        "})\n",
        "model_report"
      ],
      "metadata": {
        "id": "rkSKqAZJAt6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_report.sort_values('AUC', axis=0, ascending=False, inplace=True)\n",
        "model_report"
      ],
      "metadata": {
        "id": "iICGgMqTo_OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final prediction model is Optimal XG Boosting (XGBoost Classifier with CV and Hyperparameter Tuning).\n",
        "\n",
        "Cause, From all baseline model, Random forest classifier shows highest test accuracy and F1 score and AUC.However, an accuracy of 1.00 could be indicative of overfitting.\n",
        "\n",
        "After cross validation and hyperparameter tunning, XG Boost shows highest test accuracy score of 85% and AUC is 0.853.\n",
        "\n",
        "Cross validation and hyperparameter tunning certainly reduces chances of overfitting and also increases performance of model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we conducted a comprehensive analysis of a dataset related to credit card holders. The primary objective was to develop a predictive model to assess the likelihood of credit card default. Here are the key takeaways:\n",
        "\n",
        "\n",
        "1.   **Data Preparation and Preprocessing:** We started by understanding the dataset and performing essential data preprocessing tasks. This included handling missing values, addressing outliers, encoding categorical variables, and scaling the data.\n",
        "2.   Exploratory Data Analysis: We explored the relationships between variables and gained insights into the dataset. This involved visualizing data through various charts and plots to better understand patterns and trends.\n",
        "\n",
        "1.   M**odel Building and Evaluation:** We built several machine learning models, including Logistic Regression, Random Forest, and XG Boosting. We evaluated these models based on key performance metrics such as accuracy, precision, recall, F1 score, and AUC.\n",
        "2.   Overfitting Mitigation: We identified and addressed overfitting concerns, especially in the Random Forest model, by employing cross-validation and hyperparameter tuning.\n",
        "\n",
        "1.   **Final Model Selection:** After rigorous model evaluation, the XG Boosting model emerged as the top performer, with a test accuracy of 85% and an AUC of 0.853. This model provides a reliable estimate of credit card default likelihood.\n",
        "\n",
        "1.   **Imbalanced Dataset Handling:** We considered the issue of class imbalance and chose models that perform well in such scenarios.\n",
        "2.   **Further Analysis:** While the XG Boosting model demonstrates strong predictive capabilities, it's important to continue monitoring its performance over time and potentially refine the model as more data becomes available.\n",
        "\n",
        "Overall, this project successfully developed a predictive model to assess credit card default likelihood. The final XG Boosting model, optimized through cross-validation and hyperparameter tuning, provides a valuable tool for making informed decisions regarding credit risk.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}